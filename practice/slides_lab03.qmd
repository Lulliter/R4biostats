---
title: "Lab 3: Modeling correlation and regression"
subtitle: "<span style='font-size:2em;'> Practice session covering topics discussed in Lecture 3 </span>"
author: "<a href='https://r4biostats.com/me.html' style='color:#72aed8;font-weight:600;'>M. Chiara Mimmi, Ph.D.</a>&ensp;|&ensp;Universit√† degli Studi di Pavia"
date: 2024-07-27
date-format: long
code-link: true
format:
  revealjs:
    smaller: true
    scrollable: true
    theme: ../theme/slidesMine.scss # QUARTO LOOKS IN SAME FOLDER 
#    logo: imgs_slides/mitgest_logo.png
    footer: '[R 4 Biostatistics](https://r4biostats.com/) | MITGEST::training(2024)'
#    footer: <https://lulliter.github.io/R4biostats/lectures.html>
## ------------- x salvare come PDF 
    standalone: false
    ## -------Produce a standalone HTML file with no external dependencies,
    embed-resources: true
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    slide-number: true
    fig-cap-location: top
    # fig-format: svg
    pdf-separate-fragments: false
    # fig-align: center
execute:
  # Quarto pre code blocks do not echo their source code by default
  echo: true
  include: true
  freeze: auto
bibliography: ../bib/R4biostats.bib
csl: ../bib/apa-6th-edition.csl 
suppress-bibliography: true
---

# GOAL OF TODAY'S PRACTICE SESSION

::: {style="font-size: 85%;"}
::: {.hand .large}
[Understand .....  ]{style="color:#77501a"}
:::
:::

::: {style="font-size: 65%;"}
The examples and code from this lab session follow very closely the open book:  
+ Vu, J., & Harrington, D. (2021). **Introductory Statistics for the Life and Biomedical Sciences**. [https://www.openintro.org/book/biostat/](https://www.openintro.org/book/biostat)
:::


## Topics discussed in Lecture # 3


::: {style="font-size: 75%;"}
**Lecture 3: topics** 

+ Testing for a **correlation** hypothesis (relationship of variables)
  + Pearson rho analysis (param)
  + Spearman test (no param)
+ Measures of association 
  + Fisher‚Äôs Exact Test
  + Chi-Square Test of Independence
+ From correlation/association to **causation** 
  + introduction to experiments
    + Example: Linear regression models
    + Example: Multiple Linear Regression
+ From causation to **prediction** 
  + introduction to Machine Learning
    + Supervised algorithms
    + Unsupervised algorithms
    
:::  

# R ENVIRONMENT SET UP & DATA

## Needed R Packages
::: {style="font-size: 80%;"}
+ We will use functions from packages `base`, `utils`, and `stats` (pre-installed and pre-loaded) 
+ We will also use the packages below (specifying `package::function` for clarity).
:::

```{r}
# Load them for this R session

# General 
library(fs)      # file/directory interactions
library(here)    # tools find your project's files, based on working directory
library(janitor) # tools for examining and cleaning data
library(dplyr)   # {tidyverse} tools for manipulating and summarizing tidy data 
library(forcats) # {tidyverse} tool for handling factors
library(openxlsx) # Read, Write and Edit xlsx Files
library(flextable) # Functions for Tabular Reporting

# Statistics
library(rstatix) # Pipe-Friendly Framework for Basic Statistical Tests
library(lmtest) # Testing Linear Regression Models # Testing Linear Regression Models
library(broom) # Convert Statistical Objects into Tidy Tibbles
library(tidymodels) # not installed on this machine
# Plotting
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(ggpubr) # 'ggplot2' Based Publication Ready Plots
```


<!-- # Data  -->
<!-- # devtools::install_github("OI-Biostat/oi_biostat_data") -->
<!-- #library(oibiostat)  -->
```{r}
# load some colors
colors <- readRDS(here::here("practice", "data_input", "03_datasets","colors.rds"))
```

# DATASETS for today

 
## [Importing Dataset 1 (NHANES)]{.r-fit-text}
 
+ Adapting the function `here` to match your own folder structure
```{r}
#| eval: false
#| output: false
#| echo: false

# library(oibiostat)
# data(famuss)
# data(prevend)
# data( nhanes.samp.adult.500 )
# 
# 
# # li salvo nel mio folder per poi darglieli 
# 
# nhanes.samp.adult.500 
# write.csv(nhanes.samp.adult.500, file = here::here("practice", "data_input", "03_datasets",
#                                       "nhanes.csv"))
# 
# prevend
# write.csv(prevend, file = here::here("practice", "data_input", "03_datasets",
#                                       "prevend.csv"))
# famuss
# write.csv(famuss, file = here::here("practice", "data_input", "03_datasets",
#                                       "famuss.csv"))

# data(COL)
# colors <- COL 
# saveRDS(colors, file = here::here("practice", "data_input", "03_datasets",
#                                        "colors.rds"))

```

**Name**: NHANES (National Health and Nutrition Examination Survey) combines interviews and physical examinations to assess the health and nutritional status of adults and children in the United States. Sterted in the 1960s, it became a continuous program in 1999. 
**Documentation**: [dataset1](https://wwwn.cdc.gov/nchs/nhanes/Default.aspx)  
**Sampling details**: Here we use a sample of 500 adults from NHANES 2009-2010 & 2011-2012 (`nhanes.samp.adult.500` in the R `oibiostat` package, which has been adjusted so that it can be viewed as a random sample of the US population)
```{r}
# Check my working directory location
# here::here()

# Use `here` in specifying all the subfolders AFTER the working directory 
nhanes_samp <- read.csv(file = here::here("practice", "data_input", "03_datasets",
                                      "nhanes_samp.csv"), 
                          header = TRUE, # 1st line is the name of the variables
                          sep = ",", # which is the field separator character.
                          na.strings = c("?","NA" ), # specific MISSING values  
                          row.names = NULL) 
```
 
## [*NHANES* Variables and their description]{.r-fit-text}
::: {style="font-size: 90%;"}

```{r}
#| eval: true
#| output: true
#| echo: false

nanes_desc <- tribble(
  ~Variable, ~ Type, ~Description,
"X"                ,"int", "xxxx", 
"ID"               ,"int", "xxxxx", 
"SurveyYr"         ,"chr", "yyyy_mm. Ex. 2011_12",    
"Gender"           ,"chr", "Gender (sex) of study participant coded as male or female",
"Age"              ,"int", "##", 
"AgeDecade"        ,"chr", "yy-yy es 20-29",   
# "AgeMonths"        ,"int", "### es 471", 
"Race1"            ,"chr", "Reported race of study participant: Mexican, Hispanic, White, Black, or Other",
"Race3"            ,"chr", "Reported race of study participant... Not availale for 2009-10", 

"Education"        ,"chr", "[>= 20 yro]. Ex. 8thGrade, 9-11thGrade, HighSchool, SomeCollege, or CollegeGrad.", 
"MaritalStatus"    ,"chr", "[>= 20 yro]. Ex. Married, Widowed, Divorced, Separated, NeverMarried, or LivePartner",  
"HHIncome"         ,"chr","Total annual gross income for the household in US dollars", 
"HHIncomeMid"      ,"int", "Numerical version of HHIncome derived from the middle income in each category. Ex. 12500 40000" ,   
"Poverty"          ,"dbl", "A ratio of family income to poverty guidelines. Smaller numbers indicate more poverty Ex.. 0.95 1.74 4.99" , 
"HomeRooms"        ,"int", "How many rooms are in home of study participant (counting kitchen but not bath room).", 
"HomeOwn"          ,"chr", "One of Home, Rent, or Other", 
"Work"             ,"chr", "NotWorking Working" ,
  
"Weight"           ,"dbl", "Weight in kg",
# "Length"           ,"lgl", "Recumbent length in cm. Reported for participants aged 0 - 3 years", 
# "HeadCirc"         ,"lgl", "Head circumference in cm. Reported for participants aged 0 years (0 - 6 months)", 
"Height"           ,"dbl",  "Standing height in cm. Reported for participants aged 2 years or older.",
"BMI"              ,"dbl",  "Body mass index (weight/height2 in kg/m2). Reported for participants aged 2 years or older", 
# "BMICatUnder20yrs" ,"lgl",  "Body mass index category. Reported for participants aged 2 to 19 years", 
# "BMI_WHO"          ,"chr",  "Body mass index category. Reported for participants aged 2 years or older", 
"Pulse"            ,"int",  "60 second pulse rate",
"BPSysAve"         ,"int", "Combined systolic blood pressure reading, following the procedure outlined for BPXSAR",
"BPDiaAve"         ,"int", "Combined diastolic blood pressure reading, following the procedure outlined for BPXDAR",
"BPSys1"           ,"int", "Systolic blood pressure in mm Hg ‚Äì first reading",
"BPDia1"           ,"int", "Diastolic blood pressure in mm Hg ‚Äì second reading (consecutive readings)",
"BPSys2"           ,"int", "Systolic blood pressure in mm Hg ‚Äì second reading (consecutive readings)",
"BPDia2"           ,"int", "Diastolic blood pressure in mm Hg ‚Äì second reading",
"BPSys3"           ,"int", "Systolic blood pressure in mm Hg third reading (consecutive readings)",
"BPDia3"           ,"int", "Diastolic blood pressure in mm Hg ‚Äì third reading (consecutive readings)",
"Testosterone"     ,"dbl", "Testerone total (ng/dL). Reported for participants aged 6 years or older. Not available for 2009-2010",

"DirectChol"       ,"dbl","Direct HDL cholesterol in mmol/L. Reported for participants aged 6 years or older", 
"TotChol"          ,"dbl","Total HDL cholesterol in mmol/L. Reported for participants aged 6 years or older", 
"UrineVol1"        ,"int", "Urine volume in mL ‚Äì first test. Reported for participants aged 6 years or older", 
"UrineFlow1"       ,"dbl", "Urine flow rate in mL/min ‚Äì first test. Reported for participants aged 6 years or older", 
"UrineVol2"        ,"int", "Urine volume in mL ‚Äì second test",
"UrineFlow2"       ,"dbl","Urine flow rate (urine volume/time since last urination) in mL/min ‚Äì second test",
"Diabetes"         ,"chr","Study participant told by a doctor or health professional that they have diabetes", 
"DiabetesAge"      ,"int", "Age of study participant when first told they had diabetes", 
"HealthGen"        ,"chr", "Self-reported rating of health: Excellent, Vgood, Good, Fair, or Poor Fair" , 
"DaysPhysHlthBad"  ,"int", "Self-reported # of days participant‚Äôs physical health was not good out of the past 30 days", 
"DaysMentHlthBad " ,"int", "Self-reported # of days participant‚Äôs mental health was not good out of the past 30 days", 
"LittleInterest"   ,"chr", "Self-reported # of days where participant had little interest in doing things. Among: None, Several, Majority, or AlmostAll", 
"Depressed"        ,"chr", "Self-reported # of days where participant felt down, depressed or hopeless. Among: None, Several, Majority, or AlmostAll", 
"nPregnancies"     ,"int", "# times participant has been pregnant",
"nBabies"          ,"int", "# deliveries resulted in live births", 
"PregnantNow"      ,"chr", "Pregnancy status ascertained for females 8-59 years of age", 

"Age1stBaby"       ,"int",  "Age of participant at time of first live birth",
"SleepHrsNight"    ,"int",  "Self-reported # of hours study participant gets at night on weekdays or workdays. For participants aged 16 years and older", 
"SleepTrouble"     ,"chr", "Participant [16 years and older] has had trouble sleeping. Coded as Yes or No." ,  
"PhysActive"       ,"chr", "Participant does moderate or vigorous-intensity sports, fitness or recreational activities (Yes or No)." ,  
"PhysActiveDays"   ,"int", "Number of days in a typical week that participant does moderate or vigorous intensity activity. ", 
"TVHrsDay"         ,"chr", "Number of hours per day on average participant watched TV over the past 30 days.",
"CompHrsDay"       ,"chr", "Number of hours per day on average participant used a computer or gaming device over the past 30 day", 
"TVHrsDayChild"    ,"lgl", "[2-11 yro] Number of hours per day on average participant watched TV over the past 30 days.",
"CompHrsDayChild"  ,"lgl", "[2-11 yro] Number of hours per day on average participant used a computer or gaming device over the past 30 day", 
"Alcohol12PlusYr"  ,"chr",  "Participant has consumed at least 12 drinks of any type of alcoholic beverage in any one year", 
"AlcoholDay"       ,"int",  "Average number of drinks consumed on days that participant drank alcoholic beverages", 
"AlcoholYear"      ,"int",  "[>+ 18yro] Estimated number of days over the past year that participant drank alcoholic beverages", 
"SmokeNow"         ,"chr", "Study participant currently smokes cigarettes regularly. (Yes or No)",  
"Smoke100"         ,"chr", "Study participant has smoked at least 100 cigarettes in their entire life. (Yes pr No)",  
"Smoke100n"        ,"chr", " Smoker Non-Smoker" , 
"SmokeAge"         ,"int", "Age study participant first started to smoke cigarettes fairly regularly", 
"Marijuana"        ,"chr",  "Participant has tried marijuana", 
"AgeFirstMarij"    ,"int", "Age Participant has tried marijuana first", 
"RegularMarij"     ,"chr", "Participant has been/is a regular marijuana user (used at least once a month for a year) (Yes or No)",
"AgeRegMarij"      ,"int", "Age of participant when first started regularly using marijuana", 
"HardDrugs"        ,"chr", "Participant has tried cocaine, crack cocaine, heroin or methamphetamine (Yes or No)" ,
"SexEver"          ,"chr", "Participant had had  sex (Yes or No)" ,
"SexAge"           ,"int", "Age Participant had had sex first time" ,
"SexNumPartnLife"  ,"int", "Number of opposite sex partners participant has had",
"SexNumPartYear"   ,"int", "Number of opposite sex partners over the past 12 months",
"SameSex"          ,"chr", "Participant has had any kind of sex with a same sex partne(Yes or No)" ,
"SexOrientation"   ,"chr",  "Participant‚Äôs sexual orientation One of Heterosexual, Homosexual, Bisexual")

kableExtra::kable(nanes_desc)
```

:::


## [Importing Dataset 2 (PREVEND)]{.r-fit-text}

Data from the **Prevention of Renal and Vascular End-stage Disease (PREVEND)** study, which took place in the Netherlands. The study collected various demographic and cardiovascular risk factors. This dataset is from the third survey, which participants completed in 2003-2006; data is provided for 4,095 individuals who completed cognitive testing.

**Name**: PREVEND (Prevention of REnal and Vascular END-stage Disease) is a study which took place in the Netherlands on 8,592 participants aged 28-75, with subsequent follow-ups in 1997-1998. A second survey was conducted in 2001-2003 on 6,894 participants, 5,862 completed the third survey in 2003-2006 (here measurement of cognitive function was added to the study protocol).  
**Documentation**: [dataset2](https://research.rug.nl/en/datasets/prevention-of-renal-and-vascular-end-stage-disease-prevend)  
**Sampling details**: Data from 4,095 individuals who completed cognitive testing are in the `prevend` dataset, available in the R package `oibiostat`.
```{r}
# Check my working directory location
# here::here()

# Use `here` in specifying all the subfolders AFTER the working directory 
prevend <- read.csv(file = here::here("practice", "data_input", "03_datasets",
                                      "prevend.csv"), 
                          header = TRUE, # 1st line is the name of the variables
                          sep = ",", # which is the field separator character.
                          na.strings = c("?","NA" ), # specific MISSING values  
                          row.names = NULL) 
```

## [*PREVEND* Variables and their description]{.r-fit-text}
::: {style="font-size: 90%;"}

```{r}
#| eval: true
#| output: true
#| echo: false

famuss_desc <- tribble(
  ~Variable, ~Description,
 "...", "..."
)

kableExtra::kable(famuss_desc)
```

:::

## [Importing Dataset 3 (FAMuSS)]{.r-fit-text}
**Name**: FAMuSS (Functional SNPs Associated with Muscle Size and Strength) examine the association of demographic, physiological and genetic characteristics with muscle strength -- including data on race and genotype at a specific locus on the ACTN3 gene (the "sports gene").  
**Documentation**: [dataset3](https://www.openintro.org/data/index.php?data=famuss)  
**Sampling details**: the DATASET includes 595 observations on 9 variables

```{r}
# Check my working directory location
# here::here()

# Use `here` in specifying all the subfolders AFTER the working directory 
famuss <- read.csv(file = here::here("practice", "data_input", "03_datasets",
                                      "famuss.csv"), 
                          header = TRUE, # 1st line is the name of the variables
                          sep = ",", # which is the field separator character.
                          na.strings = c("?","NA" ), # specific MISSING values  
                          row.names = NULL) 
```

## [*FAMuSS* Variables and their description]{.r-fit-text}
::: {style="font-size: 90%;"}

```{r}
#| eval: true
#| output: true
#| echo: false

famuss_desc <- tribble(
  ~Variable, ~Description,
  "X", "id",
  "ndrm.ch", "Percent change in strength in the non-dominant arm",
  "drm.ch", "Percent change in strength in the  dominant arm" ,
  "sex", "Sex of the participant", 
  "age", "Age in years", 
  "race", "Recorded as African Am (African American), Caucasian, Asian, Hispanic, Other", 
  "height", "Height in inches" , 
  "weight", "Weight in pounds" , 
  "actn3.r577x", "Genotype at the location r577x in the ACTN3 gene.", 
  "bmi", "Body Mass Index"
)

kableExtra::kable(famuss_desc)
```

:::


# CORRELATION

## [Explore relationships between two variables]{.r-fit-text}
Approaches for summarizing relationships between two variables vary depending on variable types...

- Two **numerical** variables
- Two **categorical** variables
- One **numerical** variable and one **categorical** variable

Two variables $x$ and $y$ are 

- *positively associated* if $y$ increases as $x$ increases. 
- *negatively associated* if $y$ decreases as $x$ increases.

# TWO NUMERICAL VARIABLES (NHANES)

## [Two numerical variables (plot)]{.r-fit-text}

**Height** and **weight** (taken from the `nhanes_samp` dataset) are positively associated.

+ notice we can also use the generic function `base::plot` for a simple scatter plot
```{r}
#| echo: true
#| output-location: slide

# rename for convenience
nhanes <- nhanes_samp %>% 
  janitor::clean_names()

# basis plot 
plot(nhanes$height, nhanes$weight,
     xlab = "Height (cm)", ylab = "Weight (kg)", cex = 0.8)  
```

## [Two numerical variables: correlation (with `stats::cor`)]{.r-fit-text}

**Correlation** is a numerical summary that measures the strength of a linear relationship between two variables.

 <!-- - Introduced in *OI Biostat* Section 1.6.1; details in Ch. 6. -->

 - The correlation coefficient $r$ takes on values between $-1$ and $1$.
  - The closer $r$ is to $\pm 1$, the stronger the linear association.

+ Here we compute the **Pearson rho (parametric)**, with the function `cor`  
  + notice the `use` argument to choose how to deal with missing values (in this case only using **all complete pairs**)  

```{r}
is.numeric(nhanes$height) 
is.numeric(nhanes$weight)

# using `stats` package
cor(x = nhanes$height, y =  nhanes$weight, 
    # argument for dealing with missing values
    use = "pairwise.complete.obs",
    method = "pearson")
```

## [Two numerical variables: correlation (with `stats::cor.test`)]{.r-fit-text}

+ Here we compute the **Pearson rho (parametric)**, with the function `cor.test` (the same we used for testing paired samples)
  + implicitely takes care on `NAs`  
```{r}
# using `stats` package 
cor_test_result <- cor.test(x = nhanes$height, y =  nhanes$weight, 
                            method = "pearson")

# looking at the cor estimate
cor_test_result[["estimate"]][["cor"]]
```

+ The function `ggpubr::ggscatter` gives us all in one (scatter plot + $r$ ("R")) 

```{r}
#| echo: true
#| output-location: slide
library("ggpubr") # 'ggplot2' Based Publication Ready Plots
ggpubr::ggscatter(nhanes, x = "height", y = "weight", 
                  cor.coef = TRUE, cor.method = "pearson", #cor.coef.coord = 2,
                  xlab = "Height (in)", ylab = "Weight (lb)")
```


## [Spearman rank-order correlation]{.r-fit-text} 
The **Spearman's rank-order correlation is the nonparametric version** of the `Pearson` correlation. 

Spearman's correlation coefficient, ($œÅ$, also signified by $rs$) measures the strength and direction of association between two ranked variables.
 
+ used when 2 variables have a **non-linear** relationship
+ excellent for **ordinal** data (when Pearson's is not appropriate), i.e. Likert scale items

> To compute it, we simply calculate Pearson‚Äôs correlation of the rankings of the raw data (instead of the data).

## [Spearman rank-order correlation (example)]{.r-fit-text}

Let's say we want to get Spearman's correlation with ordinal factors `Education` and `HealthGen` in the `NHANES` sample. 
I have to convert them to their underlying numeric code, to compare rankings. 

```{r}
tabyl(nhanes$education)
tabyl(nhanes$health_gen)

nhanes <- nhanes %>% 
  # reorder education
  mutate (edu_ord = factor (education, 
                            levels = c("8th Grade", "9 - 11th Grade",
                                       "High School", "Some College",
                                       "College Grad" , NA))) %>%  
  # create edu_rank 
  mutate (edu_rank = as.numeric(edu_ord)) %>% 
  # reorder health education
  mutate (health_ord = factor (health_gen, 
                            levels = c( NA, "Poor", "Fair",
                                       "Good", "Vgood",
                                       "Excellent"))) %>%
  # create health_rank 
  mutate (health_rank = as.numeric(health_ord)) 

table(nhanes$edu_ord, useNA = "ifany" )
table(nhanes$edu_rank, useNA = "ifany" )

table(nhanes$health_ord, useNA = "ifany" )
table(nhanes$health_rank,  useNA = "ifany" )
```

## [Spearman rank-order correlation (example cont.)]{.r-fit-text}
Now we can actually compute it 

+ After setting up the variables correctly (`as.numeric`), just specify `method = "spearman"`
```{r}
# using `stats` package 
cor_test_result_sp <- cor.test(x = nhanes$edu_rank,
                               y = nhanes$health_rank, 
                               method = "spearman", 
                               exact = FALSE) # removes the Ties message warning 
# looking at the cor estimate
cor_test_result_sp
#cor_test_result_sp[["estimate"]][["rho"]]
```

# TWO CATEGORICAL VARIABLES (FAMuSS)

## [Two categorical variables (plot)]{.r-fit-text}
In the `famuss` dataset, the variables `race`, and `actn3.r577x` are categorical variables.

```{r}
## genotypes as columns
genotype.race = matrix(table(famuss$actn3.r577x, famuss$race), ncol=3, byrow=T)
colnames(genotype.race) = c("CC", "CT", "TT")
rownames(genotype.race) = c("African Am", "Asian", "Caucasian", "Hispanic", "Other")

barplot(genotype.race, col = colors[c(7, 4, 1, 2, 3)], ylim=c(0,300), width=2)
legend("topright", inset=c(.05, 0), fill=colors[c(7, 4, 1, 2, 3)], 
       legend=rownames(genotype.race))
```

## [Two categorical variables (contingency table)]{.r-fit-text}

Specifically, the variable `actn3.r577x` takes on three possible levels (`CC`, `CT`, or `TT`) which indicate the distribution of genotype at location `r577x` on the `ACTN3` gene for the FAMuSS study participants.

A **contingency table** summarizes data for two categorical variables. 

+ the function `stats::addmargins` puts arbitrary Margins on Multidimensional Tables
  + The extra column & row `"Sum"` provide the *marginal totals* across each row and each column, respectively 

```{r}
# levels of actn3.r577x
table(famuss$actn3.r577x)

# contingency table to summarize race and actn3.r577x
addmargins(table(famuss$race, famuss$actn3.r577x))
```

## [Two categorical variables (contingency table prop)]{.r-fit-text}

Contingency tables can also be converted to show *proportions*. Since there are 2 variables, it is necessary to specify whether the proportions are calculated according to the row variable or the column variable.
  + using the `margin = ` argument in the `base::prop.table` function (1 indicates rows, 2 indicates columns)

```{r}
# adding row proportions
addmargins(prop.table(table(famuss$race, famuss$actn3.r577x), margin =  1))

# adding column proportions
addmargins(prop.table(table(famuss$race, famuss$actn3.r577x),margin =  2))
```

## [Chi Squared test of `independence`]{.r-fit-text}

The **Chi-squared test** is a hypothesis test used to determine whether there is a relationship between **two categorical variables**. 

  + categorical vars. can have *nominal* or *ordinal* measurement scale
  + the *observed* frequencies are compared with the *expected* frequencies and their deviations are examined.
```{r}
# Chi-squared test
# (Test of association to see if 
# H0: the 2 cat var (race  & actn3.r577x ) are independent
# H1: the 2 cat var are correlated in __some way__

tab <- table(famuss$race, famuss$actn3.r577x)
test_chi <- chisq.test(tab)
```


the obtained result (`test_chi`) is a list of objects...

::: {.callout-warning icon=false}
### {{< bi terminal-fill color=rgba(155,103,35,1.00) >}} You try...
...run `View(test_chi)` to check 
:::


## [Chi Squared test of `independence` (cont)]{.r-fit-text}

Within `test_chi` results there are:

:::: {.columns}
::: {.column width="50%"}
+ `Observed frequencies` = how often a combination occurs in our sample
```{r}
# Observed frequencies
test_chi$observed
```

:::
  
::: {.column width="50%"}
+ `Expected frequencies` = what would it be if the 2 vars were PERFECTLY INDEPENDENT  
```{r}
# Expected frequencies
round(test_chi$expected  , digits = 1 )
```
:::
  
::::

## [Chi Squared test of `independence` (results)]{.r-fit-text}

+ Recall that 
  + $H_{0}$: the 2 cat. var. are **independent**
  + $H_{1}$: the 2 cat. var. are **correlated** in some way 

+ The result of Chi-Square test represents a comparison of the above two tables (*observed* v. *expected*): 
  + p-value = 0.01286 smaller than Œ± = 0.05 so **we REJECT the null hypothesis**
  
```{r}
test_chi
```

<!-- + Additional tables (from test results) -->
<!-- ```{r} -->
<!-- # Pearson's residual -->
<!-- test_chi$residuals   -->
<!-- # Standardized residual -->
<!-- test_chi$stdres      -->
<!-- ``` -->

## [Computing Cramer's V after test of independence]{.r-fit-text}

Recall that **Crammer's V** is a way in which we can measure *effect size* of the test of independence (i.e. a measure of the **strength of association** between two nominal variables)

+ V ranges from [0 1] (the smaller V, the lower the correlation)

$$V=\sqrt{\frac{\chi^2}{n(k-1)}} $$

where:

+ $V$ denotes Cram√©r‚Äôs V 
+ $\chi^2$ is the Pearson chi-square statistic from the aforementioned test;
+ $n$ is the sample size involved in the test and
+ $k$ is the lesser number of categories of either variable

## [Computing Cramer's V after test of independence (2 ways)]{.r-fit-text}

+ ‚úçüèª By hand" first to see the steps 
```{r}
# Compute Creamer's V
 
# inputs 
chi_calc <- test_chi$statistic
n <- nrow(famuss) # N of obd 
n_r <- nrow(test_chi$observed) # number of rows in the contingency table
n_c <- ncol(test_chi$observed) # number of columns in the contingency table

# Cramer‚Äôs V
sqrt(chi_calc / (n*min(n_r -1, n_c -1)) )
```

+ üë©üèª‚Äçüíª using an R function `rstatix::cramer_v`
```{r}
# Cramer‚Äôs V 
rstatix::cramer_v(test_chi$observed)
```

A **Cramer‚Äôs V of  0.12**  indicates a relatively weak association between the two categorical variables. It suggests that while there may be some relationship between the variables, it is not particularly strong

## [Chi Squared test of `goodness of fit`]{.r-fit-text}

In this case, we are conducting a type of Pearson‚Äôs Chi-square test

+ used to test whether the observed distribution of a categorical variable differs from your expectations 
+ the statistic is based on the discrepancies between observed and expected counts


## [Chi Squared test of `goodness of fit` (example)]{.r-fit-text}

Since the participants of the FAMuSS study where volunteers at a university, they did not come from a "representative" sample of the US population 

+ The  $\chi^{2}$ test can be used to test the $H_{0}$ that the participants are racially representative of the general population

```{r}
#| echo: false
#| output: true
famuss_race <- openxlsx::read.xlsx(
  here::here("practice","data_input","03_datasets","famuss_race.xlsx") )

famuss_race %>% flextable()
```

We use the formula $\chi^{2} = \sum_{k}\frac{(Observed - Expected)^{2}}{Expected}$,  
under $H_{0}$ = the sample proportions should equal the population proportions.


## [Chi Squared test of `goodness of fit` (example)]{.r-fit-text}

```{r} 
# Subset the vectors of frequencies from the 2 rows  
observed <- c(27,  55,  467, 46)
expected <- c(76.2,  5.95, 478.38,  34.51)

# Calculate Chi-Square statistic manually 
chi_sq_statistic <- sum((observed - expected)^2 / expected) 
df <- length(observed) - 1 
p_value <- 1 - pchisq(chi_sq_statistic, df) 

# Print results 
chi_sq_statistic
df
p_value 
```

The calculated $\chi^{2}$ statistic is very large, and the `p_value` is close to 0. Hence, There is more than sufficient evidence to **reject the null hypothesis** that the sample is representative of the general population.  
A comparison of the observed and expected values (or the residuals) indicates that the largest discrepancy is with the over-representation of Asian participants.

<!-- # 1 CATEGORICAL & 1 NUMERICAL VARIABLES (???) -->

# FROM CORRELATION/ ASSOCIATION TO CAUSATION 


## Visualize the data

We are mainly looking for a "vaguely" linear shape here 

+ `ggplot2` gives us a visual confirmation via linear best fit (the **least squares regression line**), using `method = lm`, & its 95% CI 
```{r}
#| echo: true
#| output-location: slide

ggplot(nhanes, aes (x = age, 
                          y = bmi)) + 
  geom_point() + 
  geom_smooth(method = lm,  
              #se = FALSE
              )
```


## [Linear regression models]{.r-fit-text}
The `lm()` function is used to fit linear models has the following generic structure:

```{r}
#| eval: false
lm(y ~ x, data)
```

where:

+ the 1st argument `y ~ x` specifies the variables used in the model (here the model regresses a **response variable**  $y$ against an **explanatory variable** $x$. 
+ The 2nd argument `data` is used only when the dataframe name is not already specified in the first argument. 

## [Linear regression models syntax]{.r-fit-text}

The following example shows fitting a linear model that predicts **BMI** from **age (in years)** using data from `nhanes` adult sample (individuals 21 years of age or older from the NHANES data). 
```{r}
#| eval: false
#fitting linear model
lm(nhanes$bmi ~ nhanes$age)
```

```{r}
#equivalently...
lm(bmi ~ age, data = nhanes)
```
+ Running the function creates an *object* (of class `lm`) that contains several components (model coefficients, etc), either directly displayed or accessible with `summary()` notation or specific functions.


## [Linear regression models syntax]{.r-fit-text}

<!-- https://www.youtube.com/watch?v=4wS3n54Kon0 -->
We can save the model and then extract individual output elements from it using the `$` syntax 

```{r}
#| eval: true
#| output: false

# name the model object
lr_model <- lm(bmi ~ age, data = nhanes)

# extract model output elements
lr_model$coefficients
lr_model$residuals
lr_model$fitted.values
```

The command `summary` returns these elements 

+ `Call`: reminds the equation used for this regression model
+ `Residuals`: a 5 number summary of the distribution of residuals from the regression model
+ `Coefficients`:displays the estimated coefficients of the regression model and relative hypothesis testing, given for:  
  + intercept 
  + explanatory variable(s) slope

## [Linear regression models interpretation: coefficients]{.r-fit-text}

+ The model tests the null hypothesis that a coefficient is 0
+ `Coefficients` results display: estimate, std. error, t-statistic, p-value that corresponds to the t-statistic for:
  + intercept 
  + explanatory variable(s) slope
+ In regression, the population **parameter of interest** is typically the slope parameter 
  + here age doesn't appear significantly ‚â† 0 

```{r}
summary(lr_model)$coefficients 
```

## [Linear regression models interpretation: Coefficients 2 ]{.r-fit-text}

For the the estimated coefficients of the regression model, we get:

+ `Estimate` = the average increase in the response variable associated with a one unit increase in the predictor variable, (assuming all other predictor variables are held constant).
+ `Std. Error` = a measure of the uncertainty in our estimate of the coefficient.
<!-- `Std. Error` = of coefficients = `Residual Standard Error` (see below) divided by the square root of the sum of the square of that particular x variable. -->
+ `t value`  = the t-statistic for the predictor variable, calculated as (Estimate) / (Std. Error).
+ `Pr(>|t|)` = the p-value that corresponds to the t-statistic. If less than some alpha level (e.g. 0.05). the predictor variable is said to be *statistically significant*. 


## [Linear regression models outputs: fitted values]{.r-fit-text}

Here we see $\hat{y}_i$, i.e. the **fitted $y$ value for the $i$-th individual**

```{r}
fit_val <- lr_model$fitted.values
# print the first 6 elements
head(fit_val)
```

## [Linear regression models outputs: residuals]{.r-fit-text}

Here we see $e_i = y_i - \hat{y}_i$, i.e. the **residual value for the $i$-th individual**

```{r}
resid_val <- lr_model$residuals 
# print the first 6 elements
head(resid_val)
```


## [Linear regression model's fit: Residual standard error]{.r-fit-text}

+ The `Residual standard error` (an estimate of the parameter $\sigma$) tells the average distance that the observed values fall from the regression line (we are assuming constant variance). 
  + *The smaller it is, the better the model fits the dataset!*

We can compute it manually as: 

${\rm SE}_{resid}=\ \sqrt{\frac{\sum_{i=1}^{n}{(y_i-{\hat{y}}_i)}^2}{{\rm df}_{resid}}}$

```{r}
# Residual Standard error (Like Standard Deviation)

# ---  inputs 
# sample size
n =length(lr_model$residuals)
# n of parameters in the model
k = length(lr_model$coefficients)-1 #Subtract one to ignore intercept
# degrees of freedom of the the residuals 
df_resid = n-k-1
# Squared Sum of Errors
SSE =sum(lr_model$residuals^2) # 22991.19

# --- Residual Standard Error
ResStdErr <- sqrt(SSE/df_resid)  # 6.815192
ResStdErr
```


## [Linear regression model's fit: : $R^2$ and $Adj. R^2$]{.r-fit-text} 

The **$R^2$** tells us the proportion of the variance in the response variable that can be explained by the predictor variable(s).

+ if $R^2$ close to 0 -> data more spread
+ if $R^2$ close to 1 -> data more tight around the regression line

```{r}
# --- R^2
summary(lr_model)$r.squared
```

The **$Adj. R^2$** is a modified version of R-squared that has been adjusted for the number of predictors in the model. It is always lower than the R-squared.

+ it can be useful for comparing the fit of different regression models that use different numbers of predictor variables.

```{r}
# --- Adj. R^2
summary(lr_model)$adj.r.squared
```

## [Linear regression model's fit: : F statistic]{.r-fit-text}
The **F-statistic** indicates whether the regression model provides a better fit to the data than a model that contains no independent variables. In essence, it tests if the regression model as a whole is useful.


```{r}
# extract only F statistic 
summary(lr_model)$fstatistic 

# define function to extract overall p-value of model
overall_p <- function(my_model) {
    f <- summary(my_model)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}

# extract overall p-value of model
overall_p(lr_model)

```

Given the **p-value is > 0.05**, this indicate that the predictor variable is not useful for predicting the value of the response variable

# DIAGNOSTIC PLOTS 

The following plots help us assessing that (some of) the assumptions of linear regression are met!

> (the **independence** assumption is more linked to the study design than to the data used in modeling)

<!-- 2. Independence: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data. -->

## [Linear regression diagnostic plots: residuals 1/4]{.r-fit-text}

>**ASSUMPTION 1**: there exists a linear relationship between the independent variable, x, and the dependent variable, y

For an observation $(x_i, y_i)$, where $\hat{y}_i$ is the `predicted value` according to the line $\hat{y} = b_0 + b_1x$, the `residual` is the value
$e_i = y_i - \hat{y}_i$

+ A linear lr_model is a particularly good fit for the data when the residual plot shows random scatter above and below the horizontal line.
  + (In this R plot, we look for a red line that is fairly straight)
```{r}
#| output-location: slide
#| fig-cap: "" 

# residual plot
plot(lr_model, which = 1 )
```

## [Linear regression diagnostic plots: normality of residuals 2/4]{.r-fit-text}

>**ASSUMPTION 2**: The residuals of the model are normally distributed

With the quantile-quantile plot (Q_Q) we can checking normality of the residuals.

```{r}
#| output-location: slide
#| fig-cap: "The data appear roughly normal, but there are deviations from normality in the tails, particularly the upper tail. " 

# quantile-quantile plot
plot(lr_model, which = 2 )
```


## [Linear regression diagnostic plots: Homoscedasticity 3/4]{.r-fit-text}

>**ASSUMPTION 3**: The residuals have constant variance at every level of x ("*homoscedasticity*")

This one is called a **Spread-location plot**: shows if residuals are spread equally along the ranges of predictors
```{r}
#| output-location: slide
#| fig-cap: "" 

# Spread-location plot
plot(lr_model, which = 3 )
```


## [Test for Homoscedasticity]{.r-fit-text} 

Besides visual check, we can perform the `Breusch-Pagan test` to verify the assumption of homoscedasticity. In this case:

  + $H_{0}$: residuals are distributed with **equal variance** 
  + $H_{1}$: residuals are distributed with **UNequal variance**  

+ we use `bptest` function from the `lmtest` package
 

<!--  https://www.codingprof.com/3-easy-ways-to-test-for-heteroscedasticity-in-r-examples/ -->


```{r}
# Breusch-Pagan test against heteroskedasticity 
lmtest::bptest(lr_model)

# BP = 1.5798, df = 1, p-value = 0.2088

# Because the test statistic (BP) is small and the p-value is not significant  (p-value > 0.05)  WE DO NOT REJECT THE NULL HYP
```
Because the test statistic (BP) is small and the p-value is not significant  (p-value > 0.05): WE DO NOT REJECT THE NULL HYP

## [Linear regression diagnostic plots: leverage 4/4]{.r-fit-text}

This last diagnostic plot is actually **not** referred to any assumptions but has to do with **outliers**: 

+ a **residuals vs. leverage plot** allows us to identify *influential observations* in a regression model
  + The x-axis shows the *"leverage"* of each point and the y-axis shows the "*standardized residual of each point*", i.e. "How much would the coefficients in the regression model would change if a particular observation was removed from the dataset?" 
  + Cook's distance lines (red dashed lines)  -- not visible here -- appear on the corners of the plot when there are influential cases 


```{r}
#| output-location: slide
#| fig-cap: "In this particular case, there is no influential case, or cases" 
plot(lr_model, which = 5 )
```

## [(Small digression on the `broom` package)]{.r-fit-text}

+ The `broom` package introduces the *tidy* approach to regression modeling code and outputs, allowing to convert/save them in the form of `tibbles`
+ the function `augment` will show a lot of results for the model attached to each observation 
  + this is very useful for further use of such objects, like `ggplot2` etc. 

```{r}
# render model as a dataframe 
broom::tidy(lr_model)

# see overal performance 
broom::glance(lr_model)

# save an object with all the model output elements 
model_aug <- broom::augment(lr_model)
```

::: {.callout-warning icon=false}
### {{< bi terminal-fill color=rgba(155,103,35,1.00) >}} You try...
...run `View(model_aug)` to check 
:::

# _______ >>>>> SALTO  


https://www.statology.org/interpret-regression-output-in-r/
https://www.youtube.com/watch?v=ebHLMyqC2UY
https://www.learnbymarketing.com/tutorials/explaining-the-lm-summary-in-r/

## Splitting the sample 

If we seek any causal relationship between an explanatory and outcome variable we should **split** our sample to have:

1. one `sample to "train"` a model on
2. one `sample to "test"` the model on 

+ Otherwise out model will seem better than it is (since it will be specifically built to "fit" our data) 

+ The function `rsample::initial_split` will assist in that
```{r}

nhanes_split <- rsample::initial_split(nhanes)
# default = 75%  of observations 
nhanes_train <- rsample::training(nhanes_split)
# default = 25%  of observations 
nhanes_test <- rsample::testing(nhanes_split)
```


## Linear regression performance 

We can start looking at how the model performs by applying it to our `nhanes_test` sub-sample, utilizing the function `predict`
 
```{r}
# recall the dataset size 
nrow(nhanes_test) # 125 
# obtain 125 predicted observations 
predict(lr_model, nhanes_test)
```

## Linear regression performance: predicted values in test sample

+ We can look the 95% CI of any predicted values 
```{r}
# obtain 125 predicted observations + 95% 
pred_val_test <- predict(lr_model, nhanes_test, interval = "confidence")
# print out first 6 predicted values and CI boundaries 
head(pred_val_test)
```

+ We can look the CI 95% of a single predicted values 
```{r}
# obtain 125 predicted observations + 95% 
pred_val_test <- predict(lr_model, nhanes_test, interval = "prediction")
# print out first 6 predicted values and CI boundaries 
head(pred_val_test)
```

## Linear regression performance: RMSE

Basically we are asking: **"how does the prediction compare to the actual test dataset?"**

For this we take the difference between the predicted and the actual value as 

`RMSE = Root Means Squared Error` 

```{r}
# --- testing sample 
RMSE <- sqrt(mean(
  (nhanes_test$bmi - predict(lr_model, nhanes_test))^2,
  na.rm = T))

RMSE # 7.451677
```

> This is quite close to the **Residual standard error** that we got from the regression model summary (6.843) -- despite that was taken from **training data** and this comes from testing data 

```{r}
#| eval: false
#| echo: false
#| output: false

# --- training !!!! sample 
k = length(lr_model$coefficients)-1 #Subtract one to ignore intercept
n =length(lr_model$residuals)
df = n-(1+k)
# Squared Sum of Errors
SSE =sum(lr_model$residuals**2) # 17375.27

# --- Residual Standard Error
sqrt(SSE/df)  # 6.843509
```


<!-- 
instead of dividing by df (like in Residual Standard Error) I take the SUM of SQUARE ERR divided by n(train)
so it makes sense that it is a bit higher  -->


## Linear regression performance: $R^2$

```{r}
#Multiple R-Squared (Coefficient of Determination)
SSyy=sum((nhanes_test$bmi - mean(nhanes_test$bmi, na.rm = T))**2)
SSE=sum(lr_model$residuals**2)
(SSyy-SSE)/SSyy
#Alternatively
1-SSE/SSyy
```

## Linear regression models outputs: model fit 

```{r}
str(summary(lr_model)) 
names(summary(lr_model))

summary(lr_model)$sigma
summary(lr_model)$df
summary(lr_model)$r.squared
summary(lr_model)$adj.r.squared
summary(lr_model)$fstatistic
summary(lr_model)$cov.unscaled  ##lm 
```

# _______SALTO<<<<<

